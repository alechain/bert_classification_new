{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5386553-3599-4b98-9c62-191f3aa16185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf38e7d-31ad-4830-9aba-326d56a3888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='/Users/achain/Documents/github/web-mining/pages/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24bfab1f-1544-4354-a5ac-dd07f0ad385c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plain_txt=[ data_dir+i for i in os.listdir(data_dir) if i.find('plain')!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8301adc-969d-4853-9c4d-3ff6a7612343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/achain/Documents/github/web-mining/pages/eco_content_plain',\n",
       " '/Users/achain/Documents/github/web-mining/pages/el-mun_content_plain',\n",
       " '/Users/achain/Documents/github/web-mining/pages/socie_content_plain',\n",
       " '/Users/achain/Documents/github/web-mining/pages/el-pai_content_plain']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83dead39-2c95-4a02-a8e1-93d54cf8f4bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "texts = []\n",
    "dates = []\n",
    "date_pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2})')\n",
    "\n",
    "# Iterate over folders\n",
    "for folder_path in plain_txt:\n",
    "    label = os.path.basename(folder_path)  # Extract label from folder name\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            # Extract date using regular expression\n",
    "            match = date_pattern.search(filename)\n",
    "            date = match.group(1) if match else None\n",
    "            \n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                \n",
    "                # If the title starts with \"None\", assign null date\n",
    "                if date and not text.startswith(\"None\"):\n",
    "                    labels.append(label)\n",
    "                    texts.append(text)\n",
    "                    dates.append(date)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'label': labels, 'text': texts, 'date': dates})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2ee97cd-06ad-4d28-a048-cf3b8586d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label']=df.label.str.split('_',expand=True).iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5313be5-8d27-4eb8-936d-b06ec7694114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "eco       486\n",
       "el-mun    545\n",
       "el-pai    499\n",
       "socie     544\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label')['text'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3a380f1-8ae8-4061-a4ae-59ce372752d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eco</th>\n",
       "      <td>2023-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>el-mun</th>\n",
       "      <td>2023-07-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>el-pai</th>\n",
       "      <td>2023-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>socie</th>\n",
       "      <td>2023-08-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date\n",
       "label             \n",
       "eco     2023-08-05\n",
       "el-mun  2023-07-05\n",
       "el-pai  2023-09-04\n",
       "socie   2023-08-16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label').agg({'date':'min','date':'min'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dd0a52e-71cd-415c-a951-4f9546f71d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_text=np.array([len(i) for i in df.text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c66cc431-049d-46a5-92de-a439237fc035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4130.912729026037"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_text.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e274ef57-03f7-444b-a58a-0201325084b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37619"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_text.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46db5ddc-e713-48aa-890c-da565e16ec3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_text.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea99e3f9-0a50-4f4b-956e-fa255b98d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Probamos hacer fine tunning del modelo con bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20c30719-6ca5-464c-afbd-cf3de544e2be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install transformers==4.31.0\n",
    "#!pip install torch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3e422e9-698f-4eb8-87e8-271a3adb8af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No hay GPU disponible, vamos a usar la CPU.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hay una GPU disponible?\n",
    "is_gpu = torch.cuda.is_available()\n",
    "if is_gpu:\n",
    "    # Decile a PyTorch que use la GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'\\nHay {torch.cuda.device_count()} GPU(s) disponible(s).')\n",
    "    print(f'Vamos a usar la GPU: {torch.cuda.get_device_name(0)}.\\n')\n",
    "# si no hay GPU...\n",
    "else:\n",
    "    print('\\nNo hay GPU disponible, vamos a usar la CPU.\\n')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b68c3744-7f76-4240-abe0-06155dcfe5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el tokenizador correspondiente al modelo de BERT que vamos a usar.\n",
    "# TIENE que ser exactamente el mismo.\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77f02f23-a4bf-4f42-b98a-981291204544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probamos con este modelo primero...\n",
    "## este probamos despues \"mrm8488/bert-spanish-cased-finetuned-ner\"\n",
    "# Este es el nombre en Huggingface del modelo de BERT ya entrenado que vamos a usar.\n",
    "# Ver https://huggingface.co/models?language=es&sort=downloads para mas modelos en castellano.\n",
    "BERT_MODEL = 'Recognai/bert-base-spanish-wwm-cased-xnli'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d71461b3-305f-4602-94f5-358c34793097",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando tokenizador de BERT...\n",
      "\n",
      "Tokenizador de BERT listo.\n"
     ]
    }
   ],
   "source": [
    "print('Cargando tokenizador de BERT...')\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
    "print('\\nTokenizador de BERT listo.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b54a23d-efef-42eb-85cf-dd28474c1a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c3bed84-af76-4b8c-831a-5fac11e95a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset.columns=['label', 'sentence', 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "752843f0-03c2-445b-8009-d63b7ff1334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANTE:\n",
    "# 1) Su dataset debe ser un dataframe de Pandas con 2 columnas: 'sentence' conteniendo el texto sin tokenizar, y 'label' conteniendo la categoria correspondiente.\n",
    "# 2) BERT no acepta mas de 512 tokens por frase. MY_DATASET_MAX_TOKENS es el maximo de tokens por frase que vamos a usar, el resto de descarta.\n",
    "# Si hay menos de 512 tokens el tokenizer va a llenar el resto con el token PAD.\n",
    "MY_DATASET_MAX_TOKENS = 512\n",
    "if MY_DATASET_MAX_TOKENS>512:\n",
    "  raise ValueError(f\"ERROR: BERT no puede codificar frases con mas de 512 tokens, pero MY_DATASET_MAX_TOKENS = {MY_DATASET_MAX_TOKENS}.\")\n",
    "\n",
    "# Si LAYERS_TO_FINETUNE==2, entonces conservamos los pesos de todas excepto la ultma capa de encoding y\n",
    "# hacemos finetuning the la ultima capa de encodign y  la capa de clasificacion.\n",
    "# Si es LAYERS_TO_FINETUNE>2, entonces hacemos finetuning the las ultimas LAYERS_TO_FINETUNE (hasta 12) capa de encoding y de la capa de clasificacion.\n",
    "# Si LAYERS_TO_FINETUNE==1, hacemos finetuning solo de la capa de clasificacion, entrena mas rápido pero adapta menos pesos.\n",
    "LAYERS_TO_FINETUNE=1\n",
    "if LAYERS_TO_FINETUNE<1 or LAYERS_TO_FINETUNE>13:\n",
    "  raise ValueError(\"LAYERS_TO_FINETUNE no puede ser menor a 1 o mayor a 13, poque BERT tiene 12 capas de encoder + 1 de clasificacion.\")\n",
    "\n",
    "# Que porcentaje del dataset vamos a usar para entrenar ?\n",
    "TRAIN_FOR_PCT = 0.7\n",
    "TRAIN_FOLD_SIZE = int(TRAIN_FOR_PCT * len(my_dataset))\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4\n",
    "# for over 1000 examples.\n",
    "# We chose to run for 4, but this may be over-fitting the\n",
    "# training data.\n",
    "EPOCHS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37ea910-e952-4a2a-89ae-0a3e3a31a433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3205d2-472d-469b-abd8-66a8a2e28a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "caceb041-ab81-4d45-83a3-f3f269aed276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear las listas de frases labels numéricosx\n",
    "sentences = my_dataset.sentence.values\n",
    "# labels en BERT DEBEN SER enteros; aqui transformos los valores de los labels\n",
    "# a ints (mismo int, mismo valor), comenzando por 0.\n",
    "labels, label_texts = pd.factorize(my_dataset.label)\n",
    "\n",
    "# reordenar filas del dataset al azar; importante para el descenso de gradiente estocastico\n",
    "my_dataset = my_dataset.iloc[np.random.permutation(len(my_dataset))]\n",
    "my_dataset.reset_index(drop=True)\n",
    "\n",
    "# MI_DATASET_NUM_LABELS es la cantidad de categorias distintas en su columna 'label'.\n",
    "MY_DATASET_NUM_LABELS = labels.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452dede8-1f8b-4be8-b822-da948aba62a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install protobuf==3.20.0\n",
    "#!pip install --upgrade pip\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "166381ec-dc54-41c2-94c1-aa40a62b072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# codificar frases en su dataset\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# Por cada frase ...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` va a :\n",
    "    #   (1) Tokenizar la frase.\n",
    "    #   (2) Poner el token `[CLS]` al comienzo de la frase.\n",
    "    #   (3) Poner el token `[SEP]` al final de la frase.\n",
    "    #   (4) Mapear tokens a sus IDs.\n",
    "    #   (5) Rellenar o truncar la frase a `max_length`\n",
    "    #   (6) Crear attention masks para los tokens [PAD].\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # frase a codificar.\n",
    "                        add_special_tokens = True, # Agregar '[CLS]' y '[SEP]'\n",
    "                        max_length = MY_DATASET_MAX_TOKENS,  # Rellenar frases cortas, truncar largas\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        return_token_type_ids=True,\n",
    "                        return_attention_mask = True,   # Construir attn. masks  (diferencian padding de non-padding).\n",
    "                        return_tensors = 'pt',     # retornar los vectores de pytorchxs.\n",
    "                   )\n",
    "\n",
    "    # Agregar la frase codificada a la lista de frases\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "    # Agregar su correspondiente attention mask\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convertiar las listas a tensores de Pytorch\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03d662d6-3a28-49c1-94ca-cd33412f644c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97477a26-506d-4bfc-9391-9957c63576fa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] [UNK] El Banco Central compró este jueves 9 millones de dólares en el mercado de cambios, con lo que registró 23 jornadas consecutivas con resultado positivo. La autoridad monetaria, al mismo tiempo, no modificó la tasa de interés en pesos, luego de los datos de inflación de agosto, cuando se anotó un alza del 12, 4 por ciento de aumentos en los precios minoristas. El dólar blue registró un retroceso de 10 pesos a 725 pesos por unidad. \" En septiembre, el Banco Central acumula compras por 350 millones de dólares y totaliza desde el 24 de julio más de 2. 500 millones \", según detalló Gustavo Quintana, analista de PR Corredores de Cambio. En lo referido al tipo de cambio, el dólar minorista cerró a 365, 50 pesos. En el segmento bursátil, el dólar contado con liquidación ( CCL ) bajó 0, 2 por ciento, a 739, 25 pesos [UNK] mientras que el MEP subió 0, 8 por ciento, a 681, 88 pesos, en el tramo final de la rueda. En el mercado mayorista, la divisa estadounidense finalizó con una caída de cinco centavos respecto al cierre previo, en un promedio de 350 pesos por unidad. En tanto, el Dólar Solidario y D [SEP] [UNK] Vista, el segundo productor de shale oil de Argentina, presentó su nuevo plan estratégico en el cual planea para 2026 incrementar su producción diaria en un 25 [UNK], en comparación con la meta anunciada en 2021, alcanzando así los 100. 000 barriles diarios de petróleo equivalente por día ( boe [UNK] d ). La compañía proyecta invertir USD 2. 500 millones en Vaca Muerta durante los próximos tres años, lo que representa un aumento del 60 [UNK] con respecto al monto presentado en el plan anterior. El anuncio se realizó hoy en un nuevo Investor Day de la empresa el cual fue encabezado por Miguel Galuccio, fundador, presidente y CEO de Vista, quien estuvo acompañado por el equipo ejecutivo de la compañía. Al inicio de su presentación, Galuccio destacó el innovador modelo operativo y de negocios de la compañía, que en poco tiempo la ha transformado en una empresa eficiente, rentable y con bajas emisiones. Además, mencionó que [UNK] los resultados de los primeros cinco años de actividad se deben al trabajo de un equipo y una cultura de excelencia que nos ha encaminado hacia el logro de los objetivos estratégicos establecidos en 2021 [UNK]. Galuccio confirmó que Vista alcanzó un crecimiento exponencial de su acreage en Vaca Muerta [SEP]'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## este es para ver la frase\n",
    "tokenizer.decode(tokenizer.encode_plus( sentences[0],\n",
    "                        sent,                      # frase a codificar.\n",
    "                        add_special_tokens = True, # Agregar '[CLS]' y '[SEP]'\n",
    "                        max_length = MY_DATASET_MAX_TOKENS,  # Rellenar frases cortas, truncar largas\n",
    "                        padding='max_length',\n",
    "                        truncation=True)['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a822d313-12ee-4b8c-adc0-6a6a67fcc368",
   "metadata": {},
   "source": [
    "# Cargamos el BERT en español pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc8c7164-7f27-4fe8-aa69-0c514800cdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando pesos de modelo BERT pre-entrenado...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b662d80486104938802a140879bcc7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/439M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Recognai/bert-base-spanish-wwm-cased-xnli and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "El modelo BERT pre-entrenado está listo.\n"
     ]
    }
   ],
   "source": [
    "# Cargar modelo pre-entrenado\n",
    "\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Cargar un BertForSequenceClassification, que es un modelo de BERT pre entrenado\n",
    "# con una capa de clasificación al final.\n",
    "print('Cargando pesos de modelo BERT pre-entrenado...\\n')\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    BERT_MODEL, # Modelo a usar\n",
    "    num_labels=MY_DATASET_NUM_LABELS, # cant. de categorias de mi dataset\n",
    "    ignore_mismatched_sizes=True, # si el modelo pre-entrenado tiene distinta cant. de categorias que mi dataset, ignorar las categorias del pre-entrenado, total las vamos a cambiar.\n",
    "    output_attentions = False, # El modelo debe retornat attention weights?\n",
    "    output_hidden_states = False # El modelo debe retornar todos los pesos de las capas de NN?\n",
    ")\n",
    "print('\\nEl modelo BERT pre-entrenado está listo.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841e4ea3-8e7e-44a9-bc60-e7268659560c",
   "metadata": {},
   "source": [
    "# Setup del codigo para el fine tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8045409-20fd-48f3-9748-71ced716134d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño dataset de entrenamiento: 1451 muestras\n",
      "Tamaño dataset de validacion: 623 muestras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#  setup para entrenar\n",
    "\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "# Combinar todos los ejemplos de entrenamiento en un TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "# Crear un split de validacion 90-10.\n",
    "\n",
    "# Calcular la cant. de ejmplos en train y test\n",
    "train_size = int(TRAIN_FOLD_SIZE)\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Dividir el dataset seleccionado ejemplos al azar.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f'Tamaño dataset de entrenamiento: {train_size} muestras')\n",
    "print(f'Tamaño dataset de validacion: {val_size} muestras')\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# El DataLoader necesita conocer el  batch size para entrenar, asi que aquí\n",
    "# lo definimos. Para hacer fine-tuning de BERT, los autores recomienzan un\n",
    "# batch size de 16 o 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Crear los DataLoaders para los dataset de entrenamiento y validacion.\n",
    "# Usaremos ejemplos de entrenamiento ordenados al azar\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# Para validación el orde no importa, asi que lo hacemo secuencialmente\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "\n",
    "from transformers import AdamW, BertConfig\n",
    "\n",
    "#if FINETUNE_ONLY_CLASSIFICATION_LAYER:\n",
    "  # BERT is made of 3 modules: bert, dropout, and classifier\n",
    "  # here we freeze the weights of all layers but the classifier module\n",
    "#  for name, param in model.named_parameters():\n",
    "#    if 'classifier' not in name: # classifier module\n",
    "#      param.requires_grad = False\n",
    "\n",
    "# Tell pytorch to run this model on the GPU if possible\n",
    "if is_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "# optimizer and learning rate\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs].\n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a7cee-48d9-4c59-a8a4-45b6e12da5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 8 ========\n",
      "Training...\n"
     ]
    }
   ],
   "source": [
    "# loop de entrenamiento\n",
    "import numpy as np\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    \"\"\"\n",
    "    Esta funcion calcula accuracy de predicción de labels\n",
    "    \"\"\"\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    \"\"\"\n",
    "    Recibe un timestamp en segundos y retorna un string hh:mm:ss\n",
    "    \"\"\"\n",
    "    # Redonda a segundos\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Formatear como hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss,\n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# Para cada epoch...\n",
    "for epoch_i in range(0, EPOCHS):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    # llevar a cabo 1 pasada sobre todo el dataset de entrenamiento.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, EPOCHS))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Resetear la perdida (the total loss) de este epoch\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Pasar al modelo a \"training mode\". Ojo! esto setea el modo en \"training\"\n",
    "    # pero no ejecuta el  entrenamiento.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # Por cada batch de datos de entrenamiento...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader.\n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids\n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because\n",
    "        # accumulating the gradients is \"convenient while training RNNs\".\n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here:\n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        #(loss_tensor, logits)\n",
    "        trainstep_output  = model(b_input_ids,\n",
    "                             token_type_ids=None,\n",
    "                             attention_mask=b_input_mask,\n",
    "                             labels=b_labels)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value\n",
    "        # from the tensor.\n",
    "        total_train_loss += trainstep_output.loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        trainstep_output.loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "\n",
    "        # Unpack this training batch from our dataloader.\n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using\n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids\n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which\n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here:\n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            trainstep_output = model(b_input_ids,\n",
    "                                   token_type_ids=None,\n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "\n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += trainstep_output.loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = trainstep_output.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144901db-7a05-4f65-9dec-80d271f44fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
